[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Florian Lecorvaisier’s website",
    "section": "",
    "text": "Welcome to my poorly done website! I use it mostly to develop my skills in Quarto, Git, HTML and CSS, as well as to post some stuff I may find interesting."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Eco-epidemiology researcher with a particular interest in the evolution of infectious diseases."
  },
  {
    "objectID": "cv.html#work-history",
    "href": "cv.html#work-history",
    "title": "Curriculum vitæ",
    "section": "Work history",
    "text": "Work history\n   PhD Student\n\n   10/2021 - 09/2024              LBBE, Villeurbanne, France\nWorking on the evolution of diphtheria and pertussis under vaccine pressure by the mean of mathematical modeling, numerical simulations and statistical analyzes.\n\n \n   Master degree internship\n\n   01/2021 - 06/2021              LBBE, Villeurbanne, France\nDeveloping a SIR-like model to study how toxoid vaccines could select for toxin-deficient strains.\n\n \n   Bachelor degree internship\n\n   06/2019                        LEHNA, Villeurbanne, France\nDeveloping a database of amphibian dispersal distances and morphological traits."
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum vitæ",
    "section": "Skills",
    "text": "Skills\n\n\n\n\n\n\nProgramming\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\nLaTeX\n\n\n\nPython\n\n\n\nGit & GitHub\n\n\n\nBash\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperating systems\n\n\n\n\n\n\n\n\n\n\n\n\nWindows\n\n\n\nLinux (Ubuntu)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware & Tools\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio\n\n\n\nMicrosoft Office\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguages\n\n\n\n\n\n\n\n\n\n\n\n\nFrench\n\n\n\nEnglish\n\n\n\nSpanish\n\n\n\nJapanese"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "Curriculum vitæ",
    "section": "Publications",
    "text": "Publications\n   Etude sur la compétition intra-spécifique entre souches bactérienne dans le contexte de la vaccination de masse : approches théorique et statistique\n\n   Lecorvaisier F.\n   2024                       PhD thesis\n   tel-04918453v1\n\n \n   Impact de la vaccination sur l’évolution de Bordetella pertussis\n\n   Lecorvaisier F.\n   2024                       médecine/sciences, vol. 40, n°2, pp. 161-166\n   10.1051/medsci/2023219\n\n \n   Using a dynamical model to study the impact of a toxoid vaccine on the evolution of a bacterium: The example of diphtheria\n\n   Lecorvaisier F., Pontier D., Soubeyrand B. and Fouchet D.\n   2024                       Ecological Modelling, vol. 487, pp. 110569\n   10.1051/medsci/2023219"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum vitæ",
    "section": "Education",
    "text": "Education\n   Doctoral degree                    Eco-Epidemiology\n\n   10/2021 - 12/2024              University Claude Bernard Lyon 1, France\n\n \n   University Degree                    Evolutionary Biology & Medicine\n\n   10/2021 - 09/2022              University Claude Bernard Lyon 1, France\n\n \n   Master of Science                    Ecology & Evolutionary Biology\n\n   09/2019 - 06/2021              University Claude Bernard Lyon 1, France\n\n \n   Bachelor of Science                    Ecology & Evolutionary Biology\n\n   09/2016 - 06/2021              University Claude Bernard Lyon 1, France"
  },
  {
    "objectID": "cv.html#certificates",
    "href": "cv.html#certificates",
    "title": "Curriculum vitæ",
    "section": "Certificates",
    "text": "Certificates"
  },
  {
    "objectID": "cv.html#skills-certificates",
    "href": "cv.html#skills-certificates",
    "title": "Curriculum vitæ",
    "section": "Skills & Certificates",
    "text": "Skills & Certificates\n\n\n\n\n\n\nProgramming\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\nLaTeX\n\n\n\nPython\n\n\n\nGit & GitHub\n\n\n\nBash\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperating systems\n\n\n\n\n\n\n\n\n\n\n\n\nWindows\n\n\n\nLinux (Ubuntu)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware & Tools\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio\n\n\n\nMicrosoft Office\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguages\n\n\n\n\n\n\n\n\n\n\n\n\nFrench\n\n\n\nEnglish\n\n\n\nSpanish\n\n\n\nJapanese"
  },
  {
    "objectID": "cv.html#conferences-and-presentations",
    "href": "cv.html#conferences-and-presentations",
    "title": "Curriculum vitæ",
    "section": "Conferences and presentations",
    "text": "Conferences and presentations\n  Rencontres du GdR Phylodynamique\n\n   2023                           Villeurbanne, France\n\n \n  13th International Bordetella Symposium\n\n   2022                           Vancouver, Canada\n\n \n  EvoLyon\n\n   2021                           Lyon, France"
  },
  {
    "objectID": "cv.html#achievements-honours-and-awards",
    "href": "cv.html#achievements-honours-and-awards",
    "title": "Curriculum vitæ",
    "section": "Achievements, honours and awards",
    "text": "Achievements, honours and awards\n   3-years grant for funding a thesis\n\n   2021                           E2M2 doctoral school"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My blog",
    "section": "",
    "text": "How to name Marie Curie?\n\n\n\n\n\n\nmiscellaneous\n\n\n\nHow must we name Marie Curie? I took some time to research how the two Nobel prizes winner wanted to be named.\n\n\n\n\n\nJun 30, 2025\n\n\nFlorian Lecorvaisier\n\n\n\n\n\n\n\n\n\n\n\n\nSimple linear regression\n\n\n\n\n\n\ntutorial\n\n\n\nIn this post, you will learn how to decipher the outputs of a simple linear regression model done using R.\n\n\n\n\n\nMar 31, 2025\n\n\nFlorian Lecorvaisier\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/simple-linear-regression/index.html",
    "href": "posts/simple-linear-regression/index.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "library(ggplot2)\nlibrary(patchwork)\nThis work relies heavily on Montgomery, Peck, and Vining (2021). To give the authors proper credit, they are cited multiple times throughout the document."
  },
  {
    "objectID": "posts/simple-linear-regression/index.html#introduction",
    "href": "posts/simple-linear-regression/index.html#introduction",
    "title": "Simple linear regression",
    "section": "Introduction",
    "text": "Introduction\nThe goal of the present document is to propose an introduction to the simple linear regression and its application in R. While many tutorials are available online on how to perform such a simple analysis, I found it quite hard to have an explanation not on how to do such analyzes but how they work. So this document is for all those who would like to know more about how stuff works."
  },
  {
    "objectID": "posts/simple-linear-regression/index.html#what-is-a-simple-linear-regression-model",
    "href": "posts/simple-linear-regression/index.html#what-is-a-simple-linear-regression-model",
    "title": "Simple linear regression",
    "section": "What is a simple linear regression model?",
    "text": "What is a simple linear regression model?\nThere are four words in the term “simple linear regression model”, and each has its own importance. Here is a simple explanation for each of them, from the right to the left:\n\nA model is any way to try to approach some reality using some sort of proxy. We often think about models through a mathematical point of view but models can be more diverse than that. For example, mice are often used as models to understand more broadly how the immune system of vertebrates (including humans) works.\nIn mathematics, the term regression indicates that we want to understand the link between one or multiple variables, which we often call the independent or explanatory variables, and another one called the dependent or response variable.\nThe term linear indicates that the link between the response variable and the explanatory variable(s) follows a straight line.\nFinally, we call these models simple when there is only one explanatory variable.\n\nBy convention, in most textbooks and classes the explanatory variable will be noted \\(x\\) and the response variable will be noted \\(y\\). The simple linear regression model is thus written\n\\[\ny=\\beta_0+\\beta_1x + \\varepsilon\n\\]\nwhere \\(\\beta_0\\) is the intercept of the regression, \\(\\beta_1\\) is the slope of the regression, and \\(\\varepsilon\\) is the statistical error, i.e., the variability in \\(y\\) that cannot be explained by \\(x\\). This error is here to take into account the fact that the relation between \\(x\\) and \\(y\\) is most of the time not perfectly linear.\nAnother way of writing a simple linear regression model (which I will now call SLRM - and pronounce “slurm” when I read it in my head) is\n\\[\ny_i=\\beta_0+\\beta_1x_i + \\varepsilon_i\n\\]\nwhere \\(i\\) indicates the \\(i\\)-th value for \\(x\\), \\(y\\) or \\(\\varepsilon\\). Montgomery, Peck, and Vining (2021) define the first formula as a population model and the second one as a sample regression model. The nuance is subtle and not of much interest here, but let’s say that the sample represents, as its name suggests, a fraction of the population. Most of the time, we only have access to a sample and not the whole population, and this will thus change some of the calculations shown below."
  },
  {
    "objectID": "posts/simple-linear-regression/index.html#complete-example",
    "href": "posts/simple-linear-regression/index.html#complete-example",
    "title": "Simple linear regression",
    "section": "Complete example",
    "text": "Complete example\n\nData\nFor the rest of this document, we will work on one particular example using the “rocket propellant data” proposed page 15 of Montgomery, Peck, and Vining (2021). Of course, the outcomes and results presented here should work just as fine with any other data set, including but not limited to those included in the different R packages you may find. The description of the data set is reproduced below\n\nA rocket motor is manufactured by bonding an igniter propellant and a sustainer propellant together inside a metal housing. The shear strength of the bond between the two types of propellant is an important quality characteristic. It is suspected that shear strength is related to the age in weeks of the batch of sustainer propellant.\n\nIn this example, the explanatory variable is the age of the sustainer propellant. This age, in weeks, has been measured twenty times and the values are reported below in the x variable.\n\nx = c(15.50, 23.75,  8.00, 17.00,  5.50, 19.00, 24.00,  2.50,  7.50, 11.00, \n      13.00,  3.75, 25.00,  9.75, 22.00, 18.00,  6.00, 12.50,  2.00, 21.50)\n\nOn the other hand, the response variable is the shear strength, measured in psi. For each propellant age, a corresponding shear strength has been measured, so we also have twenty values, reported below in the y variable.\n\ny = c(2158.70, 1678.15, 2316.00, 2061.30, 2207.50, 1708.30, 1784.70, 2575.00, 2357.90, 2256.70,\n      2165.20, 2399.55, 1779.80, 2336.75, 1765.30, 2053.50, 2414.40, 2200.50, 2654.20, 1753.70)\n\nA simple representation of the data can be found on Figure 1. A simple looks shows what we will demonstrate later: there appears to be a negative linear relation between the shear strength and the age of the propellant. In other words, it seems plausible that \\(\\beta_1&lt;0\\).\n\n\nCode\nggplot() +\n  geom_point(aes(x = x, y = y)) +\n  labs(x = \"Age of propellant (weeks)\", y = \"Shear strength (psi)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1: Scatter diagram of shear strength versus propellant age.\n\n\n\n\n\n\n\nSLRM using R\n\n\n\n\n\n\nNote\n\n\n\nIn this document, I will try to show how things work with a detailed step-by-step approach most of the time, but please take note that most of the time what I will show in multiple code lines can be done in one unique line without loss of readability.\n\n\nLet start by defining the formula of the SLRM we want to study. In R, it can be stored as a formula object where we link the y variable and the x variable with the use of a ~ character.\n\nslrm &lt;- y ~ x\ncat(class(slrm))\n\nformula\n\n\nThe simplest way, in R, to test the regression of \\(y\\) by \\(x\\), is to simply use the lm() function, as shown below.\n\nmod &lt;- lm(formula = slrm)\n\nThen, the summary() function can be used to produce a “human-friendly” presentation of the results of the regression.\n\nsummary(mod)\n\n\nCall:\nlm(formula = slrm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-215.98  -50.68   28.74   66.61  106.76 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2627.822     44.184   59.48  &lt; 2e-16 ***\nx            -37.154      2.889  -12.86 1.64e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 96.11 on 18 degrees of freedom\nMultiple R-squared:  0.9018,    Adjusted R-squared:  0.8964 \nF-statistic: 165.4 on 1 and 18 DF,  p-value: 1.643e-10\n\n\nThere are a lot of lines in this output so let’s review them. The first two lines indicate\nCall:\nlm(formula = slrm)\nThey show the formula used in the model. Here, it is not really informative, for two reasons. The first one is that we now what the formula is, since we are the ones who wrote it. But note that it can be useful if, for example, you are reading some outputs without having access to the code which produced them. The second reason why it is not informative here is that we stored the formula in the slrm object, thus masking what the formula really is.\nThen we have\nResiduals:\n    Min      1Q  Median      3Q     Max \n-215.98  -50.68   28.74   66.61  106.76\nThese lines give some information about the residuals of the regression. The residuals are the difference between the observed values \\(y_i\\) and the expected values \\(\\hat y_i\\). I will come back to these residuals later. The value under Min indicates the lowest value among the residuals, the 1Q value indicates the first quartile of the distribution of the residuals, the Median value indicates the median value of the residuals, the 3Q value indicates the third quartile of the distribution of the residuals and the Max value indicates the highest value among the residuals.\nThe following lines are certainly the most interesting for anyone trying to analyze any data set using a SLRM. We have\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2627.822     44.184   59.48  &lt; 2e-16 ***\nx            -37.154      2.889  -12.86 1.64e-10 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\nThis table gives different information about the estimation of the coefficients of the SLRM. Once again, we will come back to it later. The first line, starting with (Intercept), gives us the information about the intercept of the regression, while the second line, starting with x, gives us the information about the slope (i.e., the coefficient associated with x). The first column, Estimate, gives us the estimated value for each coefficient; the second, Std. Error, gives us the standard errors associated with these estimates; the third one, t value, gives us the value of the \\(t\\) statistics calculated for these estimates; and finally the last column, Pr(&gt;|t|) gives us the \\(p\\)-value associated with the \\(t\\) statistics. The asterisks shown after this last column refer to the bottom line (starting with Signif. codes:). They are used to give a visual information about the “range” of the \\(p\\)-values: if they are between 0 and 0.001 there are three asterisks; between 0.001 and 0.01 there are two asterisks; etc. It may be useful to make decisions regarding the results of your analyzes (rejecting or not your null hypothesis).\nFinally, we have a bunch of information reunited in the last three lines, as we have\nResidual standard error: 96.11 on 18 degrees of freedom\nMultiple R-squared:  0.9018,    Adjusted R-squared:  0.8964 \nF-statistic: 165.4 on 1 and 18 DF,  p-value: 1.643e-10\nThe first of these line gives the standard error of the residuals with the associated degrees of freedom. The second line shows two coefficients of determination associated with the regression. Finally, the last line indicates the results of an analysis of variance applied to the model.\n\n\nLes mains dans le cambouis1\nThe goal of this part is now to show you how to reproduce all the results R gave you automatically when you used the lm() then the summary() functions.\n\nCoefficients\n\nEstimates\nFirst, we will look at how the values in the coefficients table were obtained, and more precisely, how the estimates are calculated. The first estimate value that we can calculate is the slope. By convention, in statistics, estimates are written with a hat symbol on them. The estimate of the slope, which is the estimate of the coefficient associated with \\(x\\), is thus \\(\\hat\\beta_1\\). According to equation (2.11) in Montgomery, Peck, and Vining (2021), the least-square estimator2 of the slope is\n\\[\n\\hat\\beta_1=\\frac{S_{xx}}{S_{xy}}\n\\]\nwhere \\(S_{xx}\\) is the corrected sum of squares of the \\(x_i\\) and \\(S_{xy}\\) is the corrected sum of cross products of \\(x_i\\) and \\(y_i\\).\nAccording to equation (2.9) in Montgomery, Peck, and Vining (2021), we have\n\\[\nS_{xx}=\\sum_{i=1}^n\\left(x_i-\\bar x\\right)^2\n\\]\nand, according to equation (2.10), we have\n\\[\nS_{xy}=\\sum_{i=1}^ny_i\\left(x_i-\\bar x\\right)\n\\]\nwhere \\(n\\) is the number of observations (twenty in our example), and \\(\\bar x\\) is the mean of \\(x_i\\). This mean value can be calculated with the formula\n\\[\n\\bar x=\\frac{1}{n}\\sum_{i=1}^nx_i\n\\]\nLet’s calculate all these values using R most basic functions. The number of observations can simply be obtained using the length() function on either the x or y object.\n\nn = length(y)\n\nNow, we calculate the mean of \\(x\\).\n\nxbar = 1 / n * sum(x)\n\nNote that this value can be obtained more directly using the mean() function.\n\nxbar = mean(x)\n\nWe can now calculate the value of \\(S_{xx}\\)…\n\nSxx = sum((x - xbar) ** 2)\n\n…and the value of \\(S_{xy}\\).\n\nSxy = sum(y * (x - mean(x)))\n\nNote that these values can be obtained more directly using the var() and cov() functions. Note that these functions are tricky because they calculates unbiased estimators of the variance and covariance of the population, while we need the variance and covariance of the sample. This means that we need to correct the values given by var() and cov() with a \\(\\frac{1}{n-1}\\) factor.\n\nSxx = var(x) * (n - 1)\nSxy = cov(x, y) * (n - 1)\n\nNow that we have all that we need, we can calculate the value of \\(\\hat\\beta_1\\).\n\nbeta1hat = Sxy / Sxx\n\nIf we compare the value stored in the beta1hat object and the value given with the summary() function, we find that they are, as expected, equal.\n\ncat(summary(mod)$coefficients[2, 1])\n\n-37.15359\n\ncat(beta1hat)\n\n-37.15359\n\n\nNow that we have the value of \\(\\hat\\beta_1\\), it is easy to obtain the value of \\(\\hat\\beta_0\\). According to equation (2.6) in Montgomery, Peck, and Vining (2021), we have\n\\[\n\\hat\\beta_0=\\bar y-\\hat\\beta_1\\bar x\n\\]\nwhere \\(\\bar y\\) is the mean of \\(y_i\\). This mean value can be calculated with the formula\n\\[\n\\bar y=\\frac{1}{n}\\sum_{i=1}^ny_i\n\\]\nSo, before calculating the value of \\(\\hat\\beta_0\\), we need to calculate the value of \\(\\bar y\\).\n\nybar = 1 / n * sum(y)\n\nNote that once again this value can be obtained more directly using the mean() function.\n\nybar = mean(y)\n\nNow we can calculate the value of \\(\\hat\\beta_0\\).\n\nbeta0hat = ybar - beta1hat * xbar\n\nFor this coefficient too, if we compare the value stored in the beta0hat object and the value given with the summary() function, we find that they are equal.\n\ncat(summary(mod)$coefficients[1, 1]) \n\n2627.822\n\ncat(beta0hat)\n\n2627.822\n\n\nAnd voilà! We are all done for the values of the estimates. Now, we can continue with the second column of the coefficients table: the standard errors.\n\n\nStandard errors\nWe will first calculate the value of the standard error for the slope. According to equation (2.14) in Montgomery, Peck, and Vining (2021), the variance of \\(\\hat\\beta_1\\) is\n\\[\n\\text{var}\\left(\\hat\\beta_1\\right)=\\frac{\\sigma^2}{S_{xx}}\n\\]\nwhere \\(\\sigma^2\\) is the variance of \\(y_i\\). Unfortunately, we rarely have access to the true value of \\(\\sigma^2\\) and we must thus calculate an estimated value for it. According to equation (2.19) in Montgomery, Peck, and Vining (2021), an unbiased estimate of \\(\\sigma^2\\), \\(\\hat\\sigma^2\\), is\n\\[\n\\hat\\sigma^2 =\\frac{SS_{Res}}{n-2}\n\\]\nwhere \\(SS_{Res}\\) is the residual sum of squares and \\(n-2\\) is the number of degrees of freedom. In SLRM, the degrees of freedom is \\(n-2\\) because there are two coefficient estimates in our model: \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\). Note that \\(\\hat\\sigma^2\\) is also called the residual mean square, noted \\(MS_{Res}\\). According to equation (2.16) in Montgomery, Peck, and Vining (2021), we have\n\\[\nSS_{Res}=\\sum_{i=1}^n\\left(y_i-\\hat y_i\\right)^2\n\\]\nwhere \\(\\hat y_i\\) is the \\(i\\)-th estimated value of \\(y_i\\). The values of \\(\\hat y_i\\) can be simply obtained using the values of \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\) and \\(x_i\\). We have\n\\[\n\\hat y_i=\\hat\\beta_0+\\hat\\beta_1x_i\n\\]\nWe must thus calculate the values of \\(\\hat y_i\\) in R.\n\nyhat = beta0hat + beta1hat * x\n\nNote that the values of \\(\\hat y_i\\) can also be obtained using the predict() function on the mod object.\n\nyhat = predict(mod)\n\nNow we can calculate the value of \\(SS_{Res}\\).\n\nSSRes = sum((y - yhat) ** 2)\n\nFrom that, we can obtain the value of \\(\\hat\\sigma^2\\).\n\nsigma2hat = SSRes / (n - 2)\n\nAnd now, the value of \\(\\text{var}\\left(\\hat\\beta_1\\right)\\).\n\nvarbeta1hat = sigma2hat / Sxx\n\nFinally, to obtain the standard error reported in the coefficients table, we just need to obtain the square root of \\(\\text{var}\\left(\\hat\\beta_1\\right)\\) using the sqrt() function.\n\nsebeta1hat = sqrt(varbeta1hat)\n\nIf we compare the value stored in the sebeta1hat object and the value given with the summary() function, we find once again that they are equal.\n\ncat(sebeta1hat)\n\n2.889107\n\ncat(summary(mod)$coefficients[2, 2])\n\n2.889107\n\n\nNow let’s calculate the standard error of \\(\\hat\\beta_0\\). According to equation (2.15) from Montgomery, Peck, and Vining (2021), the variance of \\(\\hat\\beta_0\\) is\n\\[\n\\text{var}\\left(\\hat\\beta_0\\right)=\\sigma^2\\left(\\frac{1}{n}+\\frac{\\bar x^2}{S_{xx}}\\right)\n\\]\nWe will once again replace \\(\\sigma^2\\) by \\(\\hat\\sigma^2\\) as the former is not available. We thus have everything we need ready and can calculate the value of \\(\\text{var}\\left(\\hat\\beta_0\\right)\\).\n\nvarbeta0hat = sigma2hat * (1 / n + xbar ** 2 / Sxx)\n\nAnd we simply obtain the standard error of \\(\\hat\\beta_0\\) by using the sqrt() function over the varbeta0hat object.\n\nsebeta0hat = sqrt(varbeta0hat)\n\nWe can now compare the value stored in the sebeta0hat object and the value given with the summary() function and see that they are equal.\n\ncat(sebeta0hat)\n\n44.18391\n\ncat(summary(mod)$coefficients[1, 2])\n\n44.18391\n\n\n\n\n\\(t\\)-values\nWe’re progressing. Now, the third column of the coefficients table contains the \\(t\\)-values. They are used to test if the values of the coefficients are significantly different from zero, but in reality a \\(t\\) test could be used to test the difference between the estimated values of the coefficients and any theoretical value. According to equation (2.24) in Montgomery, Peck, and Vining (2021), the value of the \\(t\\) statistics for coefficient \\(\\hat\\beta_1\\) is\n\\[\nt_0=\\frac{\\hat\\beta_1-\\beta_{10}}{\\sqrt{MS_{Res}/S_{xx}}}\n\\]\nwhere \\(\\beta_{10}=0\\) in the calculations used in the results of the summary() function. Also, as a reminder, \\(MS_{Res}=\\hat\\sigma^2\\), so we can see that the denominator of the formula is simply the standard error of \\(\\hat\\beta_1\\).\nWe can thus simply calculate the value of \\(t_0\\) using the values stored in beta1hat and sebeta1hat.\n\nt0beta1hat = beta1hat / sebeta1hat\n\nWhen we compare the value in t0beta1 with the value in the coefficients table, once again, they match.\n\ncat(t0beta1hat)\n\n-12.85989\n\ncat(summary(mod)$coefficients[2, 3])\n\n-12.85989\n\n\nThe value of \\(t_0\\) for \\(\\hat\\beta_0\\) is obtained the same way.\n\nt0beta0hat = beta0hat / sebeta0hat\n\nWe compare the values here too to find that they are equal.\n\ncat(t0beta0hat)\n\n59.47464\n\ncat(summary(mod)$coefficients[1, 3])\n\n59.47464\n\n\n\n\n\\(p\\)-values\nThe \\(p\\)-values are obtained using the \\(t_0\\) values obtained above, which are supposed to follow a \\(t\\) distribution with \\(n-2\\) degrees of freedom. We use the pt() function with these values to obtain the distribution of the \\(t\\) statistics and thus the \\(p\\)-value.\nFirst, we calculate the \\(p\\)-value for coefficient \\(\\hat\\beta_1\\).\n\npbeta1hat = 2 * pt(- abs(t0beta1hat), df = n - 2)\n\nWe see that it is the same value as the one shown in the coefficients table.\n\ncat(pbeta1hat)\n\n1.643344e-10\n\ncat(summary(mod)$coefficients[2, 4])\n\n1.643344e-10\n\n\nNow, we calculate the \\(p\\)-value for coefficient \\(\\hat\\beta_0\\).\n\npbeta0hat = 2 * pt(- abs(t0beta0hat), df = n - 2)\n\nNow, it is a bit trickier than previously. If you scroll back to the global output of summary(mod) shown above, you’ll see that the \\(p\\)-value for coefficient \\(\\hat\\beta_0\\) is &lt; 2e-16. But if you print directly the value, you can see that you find another value, which is the same as the one we obtained manually.\n\ncat(pbeta0hat)\n\n4.063559e-22\n\ncat(summary(mod)$coefficients[1, 4])\n\n4.063559e-22\n\n\nBut why does R prints &lt; 2e-16 instead of the actual value? This is a bit off-topic but it is still good to know. As a computer program, R is fatally limited in the precision it can give to floats (i.e., decimal numbers). R stores a variable, called .Machine, “holding information on the numerical characteristics of the machine [it] is running on” (quote from the .Machine documentation). One of these “characteristics”, called double.eps, is the “smallest positive floating-point number \\(x\\) such that \\(1+x\\neq1\\)”. Let’s see what is the value of this \\(x\\) number.\n\ncat(.Machine$double.eps)\n\n2.220446e-16\n\n\nWe see that this value is close to the &lt; 2e-16 given by the summary() function. Globally, below this double.eps threshold, R cannot efficiently discriminate between a value different from zero and zero. We can check it easily.\n\ncat(1 + pbeta0hat != 1)\n\nFALSE\n\n\n\n\n\nResiduals\nNow that we know the values of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\), it is possible to calculate the values of the residuals. The residuals, sometimes noted \\(e_i\\), correspond to the difference between the observed values of the response variable, \\(y_i\\), and the estimated values, \\(\\hat y_i\\), as shown in equation (2.12) from Montgomery, Peck, and Vining (2021). We have\n\\[\ne_i=y_i-\\hat y_i\n\\]\nIt can easily be obtained in R with the values we alreay have calculated.\n\ne = y - yhat\n\nThe residuals can also be easily obtained using the residuals() function over the mod object.\n\ne = residuals(mod)\n\nAs a reminder, in the summary there were a few information about the residuals, mostly in the first lines. The first information we get is the minimal value of the residuals. Before accessing it, let’s have a look at the residuals. In the following code chunk, we sort (from the lowest to the highest) and print the values of the residuals.\n\nesort = sort(e)\ncat(round(esort, 2))\n\n-215.98 -213.6 -88.95 -75.32 -67.27 -45.14 -14.59 8.73 9.5 20.37 37.1 37.57 40.06 48.56 65.09 71.18 80.82 94.44 100.68 106.76\n\n\nWe immediately see that the lowest value of \\(e_i\\) is -215.98, as we saw in the summary of the SLRM. We can also access this value using the min() function.\n\nminres =  min(e)\ncat(round(minres, 2))\n\n-215.98\n\n\nThe second value is the first quartile or 25th percentile. The easiest way to obtain it is to used the quantile() function with probs = .25.\n\nQ1res = quantile(e, probs = .25)\ncat(round(Q1res, 2))\n\n-50.68\n\n\nNext, we have the median. We can access it using the median() function.\n\nmedres = median(e)\ncat(round(medres, 2))\n\n28.74\n\n\nNote that we can also access it using the quantile() function with probs = .5.\n\nmedres = quantile(e, probs = .5)\ncat(round(medres, 2))\n\n28.74\n\n\nThen we have the third quarter or 75th percentile. The easiest way to obtain it is to used the quantile() function with probs = .75.\n\nQ3res = quantile(e, probs = .75)\ncat(round(Q3res, 2))\n\n66.61\n\n\nFinally, we have the maximal value of the residuals. Looking at the esort object produced before, we can manually find that the value is 106.76. We can also obtain it using the max() function.\n\nmaxres = max(e)\ncat(round(maxres, 2))\n\n106.76\n\n\nAnd that’s everything for the top of the summary. Now, let’s have a look at the first line of the three at the bottom of the summary. The residual standard error, also called the standard error of the regression, is simply the square root of the residual mean square \\(\\hat\\sigma^2\\).\n\nsigma = sqrt(sigma2hat)\n\nWe can check that we obtain the same value as the one given in the summary.\n\ncat(sigma)\n\n96.10609\n\ncat(summary(mod)$sigma)\n\n96.10609\n\n\nNote that the \\(n-2=18\\) degrees of freedom given in the same line are the one used before, notably to calculate the value of \\(\\hat\\sigma^2\\).\n\n\nCoefficient of determination\nOn the next line out of the last three, we see two determination coefficients. A determination coefficient represents the proportion of variation in \\(y\\) explained by the regressor \\(x\\). According to equation (2.47) in Montgomery, Peck, and Vining (2021), the coefficient of determination in a SLRM is\n\\[\nR^2=1-\\frac{SS_{Res}}{SS_T}\n\\]\nwhere \\(SS_T\\) is the total variability in the observations. We have\n\\[\nSS_T=\\sum_{i=1}^n\\left(y_i-\\bar y\\right)\n\\]\nWe can easily calculate it in R.\n\nSST = sum((y - ybar) ** 2)\n\nYou can see on Figure 2 a graphical explanation of how the values of \\(SS_T\\) and \\(SS_{Res}\\) are obtained.\n\n\nCode\nSST = sum((y - ybar) ** 2)\n\nggplot() +\n  geom_hline(yintercept = ybar, linetype = \"dashed\") +\n  geom_segment(aes(x = x, y = y, yend = mean(y)), color = \"blue\") +\n  geom_point(aes(x = x, y = y)) +\n  geom_point(aes(x = x, y = ybar), color = \"blue\") +\n  labs(x = \"Age of propellant (weeks)\", y = \"Shear strength (psi)\") +\n  annotate(\"text\", x = 17.5, y = 2500, label = bquote(SS[T]==.(SST)), color = \"blue\") +\n  theme_minimal() -&gt; p1\n\nSSres = sum((y - yhat) ** 2)\n\nggplot() +\n  geom_abline(slope = coef(mod)[2], \n              intercept = coef(mod)[1], linetype = \"dashed\") +\n  labs(x = \"Age of propellant (weeks)\", y = \"Shear strength (psi)\") +\n  geom_segment(aes(x = x, y = y, yend = predict(mod)), color = \"red\") +\n  geom_point(aes(x = x, y = y)) +\n  geom_point(aes(x = x, y = yhat), color = \"red\") +\n  annotate(\"text\", x = 17.5, y = 2500, label = bquote(SS[Res]==.(SSRes)), color = \"red\") +\n  theme_minimal() -&gt; p2\n\np1 + p2 + plot_layout(axis_titles = \"collect\") + plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\n\nFigure 2: Scatter diagram of shear strength versus propellant age. (A) The dashed line follows the equation \\(y=\\bar y\\) and the blue segments indicate the distances between the observed values of \\(y\\) (black dots) and the expected values if there was no variance in \\(y\\) (blue dots). (B) The dashed line follows the equation \\(y=\\hat\\beta_0+\\hat\\beta_1x\\) and the red segments indicate the distances between the observed values of \\(y\\) (black dots) and the expected values predicted by the regression (red dots).\n\n\n\n\n\nWe can now obtain the value of \\(R^2\\).\n\nR2 = 1 - SSRes / SST\n\nWe can see that this value matches the “multiple R-squared” in the summary.\n\ncat(R2)\n\n0.9018414\n\ncat(summary(mod)$r.squared)\n\n0.9018414\n\n\nNow, how is the “adjusted R-squared” obtained? And, first of all, what is it? Well, the adjusted R-squared, which we will call \\(R^2_{Adj}\\), is most useful in multiple linear regression models than in SLRM. It can be used to compare nested models and decide if adding a new variable in the model is really useful. Without going into details, the value of \\(R^2\\) will necessarily increase when adding a new variable while the value of \\(R^2_{Adj}\\) will only increase if adding the new variable reduces the residual mean square, \\(\\hat\\sigma^2\\). According to equation (3.27) in Montgomery, Peck, and Vining (2021), we have\n\\[\nR^2_{Adj}=1-\\frac{SS_{Res}/\\left(n-p\\right)}{SS_T/\\left(n-1\\right)}\n\\]\nwhere \\(p\\) is the number of coefficients estimated in the model. In a SLRM, we have \\(p=2\\) (except in the particular case where the intercept is null by construction).\n\np = 2\n\nWe can now calculate the value of \\(R^2_{Adj}\\).\n\nR2Adj = 1 - (SSres / (n - p)) / (SST / (n - 1))\n\nAnd we find that this value is the same as the one reported in the summary.\n\ncat(R2Adj)\n\n0.8963882\n\ncat(summary(mod)$adj.r.squared)\n\n0.8963882\n\n\n\n\nAnalysis of variance\nFinally, the last line of the summary is the result of an analysis of variance with the \\(F\\)-statistic, the associated degrees of freedom, and the \\(p\\)-value. According to equation (2.36) in Montgomery, Peck, and Vining (2021), the value of the \\(F\\)-statistic, noted \\(F_0\\), is\n\\[\nF_0=\\frac{SS_R/df_R}{SS_{Res}/df_{Res}}\n\\]\nwhere \\(df_{Res}=n-2\\) and thus \\(SS_{Res}/df_{Res}=\\hat\\sigma^2\\), \\(SS_R\\) is the regression sum of squares (i.e., the variation explained by the regression) and \\(df_R=1\\) in SLRM is the associated degree of freedom. According to equation (2.34) in Montgomery, Peck, and Vining (2021), we have\n\\[\nSS_R = \\hat\\beta_1S_{xy}\n\\]\nbut we can simply note that \\(SS_R=SS_T-SS_{Res}\\). We see that \\(df_R=1\\) (first degree of freedom reported in the summary) because \\(SS_R\\) is determined solely by \\(\\hat\\beta_1\\) while \\(df_{Res}=n-2\\) (second degree of freedom reported in the summary) because both \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are used in calculating the values of \\(y_i-\\hat y_i\\) (see Montgomery, Peck, and Vining (2021) for more details).\nWe fix the values of \\(df_R\\) and \\(df_{Res}\\).\n\ndfR = 1\ndfRes = n - 2\n\nWe calculate the value of \\(SS_R\\).\n\nSSR = beta1hat * Sxy\n\nFinally, we can calculate the value of \\(F_0\\).\n\nF0 = (SSR / dfR) / (SSRes / dfRes)\n\nOnce more, if we compare the value obtained manually with the value shown in the summary, we find out that they are the same.\n\ncat(F0)\n\n165.3768\n\ncat(summary(mod)$fstatistic[1])\n\n165.3768\n\n\nThe value of \\(F_0\\) follows the \\(F_{df_R,df_{Res}}\\) distribution. From that, we can easily find the associated \\(p\\)-value using the pf() function.\n\npval = 1 - pf(F0, dfR, dfRes)\n\nThe value is the same as the one shown in the summary (actually, it is not possible to print the value directly out of the summary). Note that the \\(p\\)-value is the same as the one found for the \\(t\\)-test of coefficient \\(\\hat\\beta_1\\).\n\ncat(pval)\n\n1.643343e-10"
  },
  {
    "objectID": "posts/simple-linear-regression/index.html#conclusion",
    "href": "posts/simple-linear-regression/index.html#conclusion",
    "title": "Simple linear regression",
    "section": "Conclusion",
    "text": "Conclusion\nIn this document, I tried to explain how the different outputs given by R when working on a single linear regression model are mathematically obtained. The goal was not to demonstrate in details how the different formulas used are constructed. For those interested, I redirect them to Montgomery, Peck, and Vining (2021), which I heavily used to produce this work."
  },
  {
    "objectID": "posts/simple-linear-regression/index.html#session-info",
    "href": "posts/simple-linear-regression/index.html#session-info",
    "title": "Simple linear regression",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=French_France.utf8  LC_CTYPE=French_France.utf8   \n[3] LC_MONETARY=French_France.utf8 LC_NUMERIC=C                  \n[5] LC_TIME=French_France.utf8    \n\ntime zone: Europe/Paris\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] patchwork_1.3.1 ggplot2_3.5.2  \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.5        cli_3.6.5          knitr_1.50         rlang_1.1.6       \n [5] xfun_0.52          generics_0.1.4     jsonlite_2.0.0     labeling_0.4.3    \n [9] glue_1.8.0         htmltools_0.5.8.1  scales_1.4.0       rmarkdown_2.29    \n[13] grid_4.5.1         evaluate_1.0.4     tibble_3.3.0       fastmap_1.2.0     \n[17] yaml_2.3.10        lifecycle_1.0.4    compiler_4.5.1     dplyr_1.1.4       \n[21] RColorBrewer_1.1-3 htmlwidgets_1.6.4  pkgconfig_2.0.3    rstudioapi_0.17.1 \n[25] farver_2.1.2       digest_0.6.37      R6_2.6.1           tidyselect_1.2.1  \n[29] pillar_1.10.2      magrittr_2.0.3     withr_3.0.2        tools_4.5.1       \n[33] gtable_0.3.6"
  },
  {
    "objectID": "posts/simple-linear-regression/index.html#footnotes",
    "href": "posts/simple-linear-regression/index.html#footnotes",
    "title": "Simple linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLes mains dans le cambouis is a French expression meaning that you are going deep inside how something works; going beyond just using something but really understanding how it is built.↩︎\nIn linear regression models, the estimators considered as “best” are those minimizing the sum of the squares of the residuals.↩︎"
  },
  {
    "objectID": "posts/marie-curie/index.html",
    "href": "posts/marie-curie/index.html",
    "title": "How to name Marie Curie?",
    "section": "",
    "text": "Marie Curie, or Maria Salomea Skłodowska, or Marie Skłodowska-Curie, or… Well, you get it. One of the most famous scientists of all times, only winner of two Nobel prices in two different scientific fields. But when I find people online talking about her, it is mostly to debate how to write her name. I often read some interactions like this one:\n\n“Hey, here is an interesting fact about Marie Curie!”\n“Hmm actually it is Maria Skłodowska.”\n“No, she used both her Polish and French names, so it is Maria Skłodowska-Curie.”\n“Is it Marie or Maria?”\n\nSo what is the correct answer? In fact… all of it is correct. Marie Curie – I will call her Marie Curie in the rest of this post to keep it simple – did use all of these names, depending of the period (she obviously did not use the Curie surname before her marriage with Pierre Curie), who she was addressing to (French or Polish friends or colleagues), and the nature of the document (letters, books, scientific publications…). So let’s review a bit of documents signed by Marie Curie."
  },
  {
    "objectID": "posts/marie-curie/index.html#french-letters",
    "href": "posts/marie-curie/index.html#french-letters",
    "title": "How to name Marie Curie?",
    "section": "French letters",
    "text": "French letters\nBecause her family and most of her colleagues were French, most of the letters Marie Curie wrote during her life were written in French, and she used multiple signatures depending on who she was writing to. For example, in one letter to Alexander Graham Bell written in 1903, she signed “M. Curie” [link].\nThe “Archives et manuscrits” departments of the Bibliothèque nationale de France (Bnf) digitized some of the personal correspondence of Marie Curie under the archive number NAF 28138 [link]. This archive is divided in three sub-units: the letters to and from her family from 1905 to September 1914 (NAF 28138 (1)), the letters to and from her family from October 1914 to May 1934 (NAF 28138 (2)), and her letters to Eugénie Cotton (born Feytis) and Isabelle Chavannes (NAF 28138 (3)). I reviewed both the NAF 28138 (1) and NAF 28138 (3) archives, letting behind the NAF 28138 (2) one. The reason behind it is that I already reviewed 200+ family letters in the NAF 28138 (1) archive and did not have the time nor the motivation to review another set of 200+ letters from the NAF 28138 (2) archive.\nIn the letters addressed to her family, it is obvious that she did not sign with something as formal as “Marie Curie” or “Mme Skłodowska-Curie”. But it is still interesting to learn how she was referred to by her family and how she signed her letters to them. In the NAF 28138 (1) archive, most of the letters are from Irène Curie, her daughter. We discover that Irène mostly referred to her mother as “Mé” in her young years, switching to “Ma chérie” when she grew older (Table 1). Ève, the other daughter of Marie Curie, also wrote some letters, always (in the set of letters I reviewed) referring to her as “Mé” (Table 1).\nNote that, in this review, I focused on how people greeted Marie Curie at the beginning of the letters. I did not extensively searched how people referred to her in the main text of the letters. Nevertheless, I noticed that even after she started to use “Ma chérie” in most of her letters, Irène continued to name her mother “Mé” in the body of her letters [link] [link].\nI also did not formerly analyzed the addresses on envelops and postcards, but I noticed that she is often named “Mme Curie” or “Mme Marie Curie”. More rarely, she is named “Madame Pierre Curie” [link], “Me P. Curie” [link] or “Me Pierre Curie” [link] [link]. Interestingly, I saw one letter from Irène to Marie with the name “Me Marie Skłodowska” (with the correct “ł”) [link] and three others with “Me Skłodowska” [link] [link] [link].\nWhen Marie Curie wrote to her daughters, she had two ways to sign. She either used “Mé”, as her daughters called her, or “Ta mère” (“Your mother”) (Table 1).\n\n\n\nTable 1: Names given to Marie Curie in greeting or signatures in the NAF 28138 (1) archive.\n\n\n\n\n\nFrom\nTo\nName for Marie Curie\nLink(s)\n\n\n\n\nIrène Curie\nMarie Curie\nMé (with variable affectionate adjectives)\n[link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link1] [link1] [link] [link1]\n\n\nIrène Curie\nMarie Curie\nChérie (with variable affectionate adjectives or just “Ma”)\n[link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link2] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link]\n\n\nEve Curie\nMarie Curie\nMé\n[link] [link] [link] [link]\n\n\nMarie Curie\nIrène Curie\nMé\n[link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link]\n\n\nMarie Curie\nIrène Curie\nTa mère\n[link] [link] [link] [link] [link] [link] [link] [link] [link] [link3] [link3] [link] [link] [link] [link] [link] [link] [link]\n\n\nMarie Curie\nIrène Curie\nM.\n[link4]\n\n\nMarie Curie\nEve Curie\nMé\n[link]\n\n\nIsabelle Chavannes\nMarie Curie\nChère Madame\n[link]\n\n\n\n\n\n\n1 “Ma douce Mé chérie”.\n2 Not signed but we can guess the letter is from Irène.\n3 Addressed to both Irène and Eve and signed “Votre mère”.\n4 Could be “Mé”, it is possible that I misread the signature.\nIn the NAF 28138 (3) archive, Marie Curie wrote to two students, colleagues and friends of hers: Eugénie Cotton (born Feytis) and Isabelle Chavannes. In these 30 letters, she always signed as “M. Curie”, where “M.” stands for “Marie” (Table 2).\n\n\n\nTable 2: Name used by Marie Curie to sign her letters in the NAF 28138 (3) archive.\n\n\n\n\n\nFrom\nTo\nName for Marie Curie\nLink(s)\n\n\n\n\nMarie Curie\nEugénie Feytis\nM. Curie\n[link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link]\n\n\nMarie Curie\nIsabelle Chavannes\nM. Curie\n[link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link]"
  },
  {
    "objectID": "posts/marie-curie/index.html#english-letters",
    "href": "posts/marie-curie/index.html#english-letters",
    "title": "How to name Marie Curie?",
    "section": "English letters",
    "text": "English letters\nUnfortunately, I found only a few letters from Marie Curie written in English. We know that she exchanged 32 letters with Albert Einstein, but I could not access to any written by Curie. These letters have been published by the “Muzeum Marii Skłodowskiej-Curie w Warszawie” but I could not access the book, and I am not sure that the letters wouldn’t be translated to Polish [link]."
  },
  {
    "objectID": "posts/marie-curie/index.html#polish-letters",
    "href": "posts/marie-curie/index.html#polish-letters",
    "title": "How to name Marie Curie?",
    "section": "Polish letters",
    "text": "Polish letters\nI managed to find two letters from Marie Curie written in Polish. One is addressed to a friend of hers, where she announce that she will not be able to visit her in Switzerland because she has to go to Stockholm to get her second Nobel price. In this document, she signs as “M. Curie” [link]. The other letter is addressed to Jan Moszczeńskiego, president of the Polish comity of New York. In this letter, she signs as “M. Skłodowska Curie” [link]."
  }
]